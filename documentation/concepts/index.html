<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Concepts • Vitess</title>
    <meta name="description" content="Concepts

We need to introduce some common terminologies that are used in Vitess:

Keyspace

A keyspace is a logical database.
In its simplest form, it directly maps to a MySQL database name.
When you read data from a keyspace, it is as if you read from a MySQL database.
Vitess could fetch that data from a master or a replica depending
on the consistency requirements of the read.

When a database gets sharded,
a keyspace maps to multiple MySQL databases,
and the necessary data is fetched from one of the shards.
Reading from a keyspace gives you the impression that the data is read from
a single MySQL database.

Shard

A division within a Keyspace. All the instances inside a Shard have the same data (or should have the same data,
modulo some replication lag).

A Keyspace usually has one shard when not using any sharding (we name it &#39;0&#39; by convention). When sharded, a Keyspace will have N shards (usually, N is a power of 2) with non-overlapping data.

We support dynamic resharding, when one shard is split into 2 shards for instance. In this case, the data in the
source shard is duplicated into the 2 destination shards, but only during the transition. Afterwards, the source shard is
deleted.

A shard usually contains one MySQL master, and many MySQL slaves. The slaves are used to serve read-only traffic (with
eventual consistency guarantees), run data analysis tools that take a long time to run, or perform administrative tasks (backups, restore, diffs, ...)

Tablet

A tablet is a single server that runs:
- a MySQL instance
- a vttablet instance
- a local row cache instance
- an other per-db process that is necessary for operational purposes

It can be idle (not assigned to any keyspace), or assigned to a keyspace/shard. If it becomes unhealthy, it is usually changed to scrap.

It has a type. The commonly used types are:
- master: for the mysql master, RW database.
- replica: for a mysql slave that serves read-only traffic, with guaranteed low replication latency.
- rdonly: for a mysql slave that serves read-only traffic for backend processing jobs (like map-reduce type jobs). It has no real guaranteed replication latency.
- spare: for a mysql slave not use at the moment (hot spare).
- experimental, schema, lag, backup, restore, checker, ... : various types for specific purposes.

Only master, replica and rdonly are advertised in the Serving Graph.

Keyspace id

A keyspace id (keyspace_id) is a column that is used to identify a primary entity
of a keyspace, like user, video, order, etc.
In order to shard a database, all tables in a keyspace need to
contain a keyspace id column.
Vitess sharding ensures that all rows that have a common keyspace id are
always together.

It&#39;s recommended, but not necessary, that the keyspace id be the leading primary
key column of all tables in a keyspace.

If you do not intend to shard a database, you do not have to
designate a keyspace_id.
However, you&#39;ll be required to designate a keyspace_id
if you decide to shard a currently unsharded database.

A keyspace_id can be an unsigned number or a binary character column (unsigned bigint
or varbinary in mysql tables). Other data types are not allowed because of ambiguous
equality or inequality rules.

TODO: The keyspace id rules need to be solidified once VTGate features are finalized.

Shard graph

The shard graph defines how a keyspace has been sharded. It&#39;s basically a per-keyspace
list of non-intersecting ranges that cover all possible values a keyspace id can cover.
In other words, any given keypsace id is guaranteed to map to one and only one
shard of the shard graph.

We are going with range based sharding.
The main advantage of this scheme is that the shard map is a simple in-memory lookup.
The downside of this scheme is that it creates hot-spots for sequentially increasing keys.
In such cases, we recommend that the application hash the keys so they
distribute more randomly.

For instance, an application may use an incrementing UserId as a primary key for user records,
and a hashed version of that UserId as a keyspace_id. All data related to one user will be on
the same shard, as all rows will share that keyspace_id.

Replication graph

The Replication Graph represents the relationships between the master
databases and their respective replicas.
This data is particularly useful during a master failover.
Once a new master has been designated, all existing replicas have to
repointed to the new master so that replication can resume.

Serving graph

The Serving Graph is derived from the shard and replication graph.
It represens the list of active servers that are available to serve
queries.
VTGate (or smart clients) query the serving graph to find out which servers
they are allowed to send queries to.

Topology Server

The Topology Server is the backend service used to store the Topology data, and provide a locking service. The implementation we use in the tree is based on Zookeeper. Each Zookeeper process is run on a single server, but may share that server with other processes.

There is a global instance of that service. It contains data that doesn&#39;t change often, and references other local instances. It may be replicated locally in each Data Center as read-only copies. (a Zookeeper instance with two master instances per cell and one or two replicas per cell is a good configuration).

There is one local instance of that service per Cell (Data Center). The goal is to transparently support a Cell going down. When that happens, we assume the client traffic is drained out of that Cell, and the system can survive
using the remaining Cells. (a Zookeeper instance running on 3 or 5 hosts locally is a good configuration).

The data is partitioned as follows:
- Keyspaces: global instance
- Shards: global instance
- Tablets: local instances
- Serving Graph: local instances
- Replication Graph: the master alias is in the global instance, the master-slave map is in the local cells.

Clients usually just read the local Serving Graph, therefore they only need the local instance to be up. Also, we provide a caching layer for Zookeeper, to survive local Zookeeper failures and scale read-only access dramatically.

Cell (Data Center)

A Cell is a group of servers and network infrastructure collocated in an area. It is usually a full Data Center, or a subset of a full Data Center.

A Cell has an associated Topology Server, hosted in that Cell. Most information about the tablets in a cell is hosted in that cell&#39;s Topology Server. That way a Cell can be taken down and rebuilt as a unit, for instance.

We try to limit cross-cell traffic (both for data and metadata), and gracefully handle cell-level failures (like a Cell being cut off the network). Having the ability to route client traffic to Cells individually is a great feature to have
(but not provided by the Vitess software).
">
    <meta name="keywords" content="">
    
    
    	<!-- Twitter Cards -->
	<meta name="twitter:title" content="Concepts">
	<meta name="twitter:description" content="Concepts

We need to introduce some common terminologies that are used in Vitess:

Keyspace

A keyspace is a logical database.
In its simplest form, it directly maps to a MySQL database name.
When you read data from a keyspace, it is as if you read from a MySQL database.
Vitess could fetch that data from a master or a replica depending
on the consistency requirements of the read.

When a database gets sharded,
a keyspace maps to multiple MySQL databases,
and the necessary data is fetched from one of the shards.
Reading from a keyspace gives you the impression that the data is read from
a single MySQL database.

Shard

A division within a Keyspace. All the instances inside a Shard have the same data (or should have the same data,
modulo some replication lag).

A Keyspace usually has one shard when not using any sharding (we name it &#39;0&#39; by convention). When sharded, a Keyspace will have N shards (usually, N is a power of 2) with non-overlapping data.

We support dynamic resharding, when one shard is split into 2 shards for instance. In this case, the data in the
source shard is duplicated into the 2 destination shards, but only during the transition. Afterwards, the source shard is
deleted.

A shard usually contains one MySQL master, and many MySQL slaves. The slaves are used to serve read-only traffic (with
eventual consistency guarantees), run data analysis tools that take a long time to run, or perform administrative tasks (backups, restore, diffs, ...)

Tablet

A tablet is a single server that runs:
- a MySQL instance
- a vttablet instance
- a local row cache instance
- an other per-db process that is necessary for operational purposes

It can be idle (not assigned to any keyspace), or assigned to a keyspace/shard. If it becomes unhealthy, it is usually changed to scrap.

It has a type. The commonly used types are:
- master: for the mysql master, RW database.
- replica: for a mysql slave that serves read-only traffic, with guaranteed low replication latency.
- rdonly: for a mysql slave that serves read-only traffic for backend processing jobs (like map-reduce type jobs). It has no real guaranteed replication latency.
- spare: for a mysql slave not use at the moment (hot spare).
- experimental, schema, lag, backup, restore, checker, ... : various types for specific purposes.

Only master, replica and rdonly are advertised in the Serving Graph.

Keyspace id

A keyspace id (keyspace_id) is a column that is used to identify a primary entity
of a keyspace, like user, video, order, etc.
In order to shard a database, all tables in a keyspace need to
contain a keyspace id column.
Vitess sharding ensures that all rows that have a common keyspace id are
always together.

It&#39;s recommended, but not necessary, that the keyspace id be the leading primary
key column of all tables in a keyspace.

If you do not intend to shard a database, you do not have to
designate a keyspace_id.
However, you&#39;ll be required to designate a keyspace_id
if you decide to shard a currently unsharded database.

A keyspace_id can be an unsigned number or a binary character column (unsigned bigint
or varbinary in mysql tables). Other data types are not allowed because of ambiguous
equality or inequality rules.

TODO: The keyspace id rules need to be solidified once VTGate features are finalized.

Shard graph

The shard graph defines how a keyspace has been sharded. It&#39;s basically a per-keyspace
list of non-intersecting ranges that cover all possible values a keyspace id can cover.
In other words, any given keypsace id is guaranteed to map to one and only one
shard of the shard graph.

We are going with range based sharding.
The main advantage of this scheme is that the shard map is a simple in-memory lookup.
The downside of this scheme is that it creates hot-spots for sequentially increasing keys.
In such cases, we recommend that the application hash the keys so they
distribute more randomly.

For instance, an application may use an incrementing UserId as a primary key for user records,
and a hashed version of that UserId as a keyspace_id. All data related to one user will be on
the same shard, as all rows will share that keyspace_id.

Replication graph

The Replication Graph represents the relationships between the master
databases and their respective replicas.
This data is particularly useful during a master failover.
Once a new master has been designated, all existing replicas have to
repointed to the new master so that replication can resume.

Serving graph

The Serving Graph is derived from the shard and replication graph.
It represens the list of active servers that are available to serve
queries.
VTGate (or smart clients) query the serving graph to find out which servers
they are allowed to send queries to.

Topology Server

The Topology Server is the backend service used to store the Topology data, and provide a locking service. The implementation we use in the tree is based on Zookeeper. Each Zookeeper process is run on a single server, but may share that server with other processes.

There is a global instance of that service. It contains data that doesn&#39;t change often, and references other local instances. It may be replicated locally in each Data Center as read-only copies. (a Zookeeper instance with two master instances per cell and one or two replicas per cell is a good configuration).

There is one local instance of that service per Cell (Data Center). The goal is to transparently support a Cell going down. When that happens, we assume the client traffic is drained out of that Cell, and the system can survive
using the remaining Cells. (a Zookeeper instance running on 3 or 5 hosts locally is a good configuration).

The data is partitioned as follows:
- Keyspaces: global instance
- Shards: global instance
- Tablets: local instances
- Serving Graph: local instances
- Replication Graph: the master alias is in the global instance, the master-slave map is in the local cells.

Clients usually just read the local Serving Graph, therefore they only need the local instance to be up. Also, we provide a caching layer for Zookeeper, to survive local Zookeeper failures and scale read-only access dramatically.

Cell (Data Center)

A Cell is a group of servers and network infrastructure collocated in an area. It is usually a full Data Center, or a subset of a full Data Center.

A Cell has an associated Topology Server, hosted in that Cell. Most information about the tablets in a cell is hosted in that cell&#39;s Topology Server. That way a Cell can be taken down and rebuilt as a unit, for instance.

We try to limit cross-cell traffic (both for data and metadata), and gracefully handle cell-level failures (like a Cell being cut off the network). Having the ability to route client traffic to Cells individually is a great feature to have
(but not provided by the Vitess software).
">
	
	
	
	<meta name="twitter:card" content="summary">
	<meta name="twitter:image" content="http://youtube.github.io/vitess/images/120x120.gif">
	
	<!-- Open Graph -->
	<meta property="og:locale" content="en_US">
	<meta property="og:type" content="article">
	<meta property="og:title" content="Concepts">
	<meta property="og:description" content="Concepts

We need to introduce some common terminologies that are used in Vitess:

Keyspace

A keyspace is a logical database.
In its simplest form, it directly maps to a MySQL database name.
When you read data from a keyspace, it is as if you read from a MySQL database.
Vitess could fetch that data from a master or a replica depending
on the consistency requirements of the read.

When a database gets sharded,
a keyspace maps to multiple MySQL databases,
and the necessary data is fetched from one of the shards.
Reading from a keyspace gives you the impression that the data is read from
a single MySQL database.

Shard

A division within a Keyspace. All the instances inside a Shard have the same data (or should have the same data,
modulo some replication lag).

A Keyspace usually has one shard when not using any sharding (we name it &#39;0&#39; by convention). When sharded, a Keyspace will have N shards (usually, N is a power of 2) with non-overlapping data.

We support dynamic resharding, when one shard is split into 2 shards for instance. In this case, the data in the
source shard is duplicated into the 2 destination shards, but only during the transition. Afterwards, the source shard is
deleted.

A shard usually contains one MySQL master, and many MySQL slaves. The slaves are used to serve read-only traffic (with
eventual consistency guarantees), run data analysis tools that take a long time to run, or perform administrative tasks (backups, restore, diffs, ...)

Tablet

A tablet is a single server that runs:
- a MySQL instance
- a vttablet instance
- a local row cache instance
- an other per-db process that is necessary for operational purposes

It can be idle (not assigned to any keyspace), or assigned to a keyspace/shard. If it becomes unhealthy, it is usually changed to scrap.

It has a type. The commonly used types are:
- master: for the mysql master, RW database.
- replica: for a mysql slave that serves read-only traffic, with guaranteed low replication latency.
- rdonly: for a mysql slave that serves read-only traffic for backend processing jobs (like map-reduce type jobs). It has no real guaranteed replication latency.
- spare: for a mysql slave not use at the moment (hot spare).
- experimental, schema, lag, backup, restore, checker, ... : various types for specific purposes.

Only master, replica and rdonly are advertised in the Serving Graph.

Keyspace id

A keyspace id (keyspace_id) is a column that is used to identify a primary entity
of a keyspace, like user, video, order, etc.
In order to shard a database, all tables in a keyspace need to
contain a keyspace id column.
Vitess sharding ensures that all rows that have a common keyspace id are
always together.

It&#39;s recommended, but not necessary, that the keyspace id be the leading primary
key column of all tables in a keyspace.

If you do not intend to shard a database, you do not have to
designate a keyspace_id.
However, you&#39;ll be required to designate a keyspace_id
if you decide to shard a currently unsharded database.

A keyspace_id can be an unsigned number or a binary character column (unsigned bigint
or varbinary in mysql tables). Other data types are not allowed because of ambiguous
equality or inequality rules.

TODO: The keyspace id rules need to be solidified once VTGate features are finalized.

Shard graph

The shard graph defines how a keyspace has been sharded. It&#39;s basically a per-keyspace
list of non-intersecting ranges that cover all possible values a keyspace id can cover.
In other words, any given keypsace id is guaranteed to map to one and only one
shard of the shard graph.

We are going with range based sharding.
The main advantage of this scheme is that the shard map is a simple in-memory lookup.
The downside of this scheme is that it creates hot-spots for sequentially increasing keys.
In such cases, we recommend that the application hash the keys so they
distribute more randomly.

For instance, an application may use an incrementing UserId as a primary key for user records,
and a hashed version of that UserId as a keyspace_id. All data related to one user will be on
the same shard, as all rows will share that keyspace_id.

Replication graph

The Replication Graph represents the relationships between the master
databases and their respective replicas.
This data is particularly useful during a master failover.
Once a new master has been designated, all existing replicas have to
repointed to the new master so that replication can resume.

Serving graph

The Serving Graph is derived from the shard and replication graph.
It represens the list of active servers that are available to serve
queries.
VTGate (or smart clients) query the serving graph to find out which servers
they are allowed to send queries to.

Topology Server

The Topology Server is the backend service used to store the Topology data, and provide a locking service. The implementation we use in the tree is based on Zookeeper. Each Zookeeper process is run on a single server, but may share that server with other processes.

There is a global instance of that service. It contains data that doesn&#39;t change often, and references other local instances. It may be replicated locally in each Data Center as read-only copies. (a Zookeeper instance with two master instances per cell and one or two replicas per cell is a good configuration).

There is one local instance of that service per Cell (Data Center). The goal is to transparently support a Cell going down. When that happens, we assume the client traffic is drained out of that Cell, and the system can survive
using the remaining Cells. (a Zookeeper instance running on 3 or 5 hosts locally is a good configuration).

The data is partitioned as follows:
- Keyspaces: global instance
- Shards: global instance
- Tablets: local instances
- Serving Graph: local instances
- Replication Graph: the master alias is in the global instance, the master-slave map is in the local cells.

Clients usually just read the local Serving Graph, therefore they only need the local instance to be up. Also, we provide a caching layer for Zookeeper, to survive local Zookeeper failures and scale read-only access dramatically.

Cell (Data Center)

A Cell is a group of servers and network infrastructure collocated in an area. It is usually a full Data Center, or a subset of a full Data Center.

A Cell has an associated Topology Server, hosted in that Cell. Most information about the tablets in a cell is hosted in that cell&#39;s Topology Server. That way a Cell can be taken down and rebuilt as a unit, for instance.

We try to limit cross-cell traffic (both for data and metadata), and gracefully handle cell-level failures (like a Cell being cut off the network). Having the ability to route client traffic to Cells individually is a great feature to have
(but not provided by the Vitess software).
">
	<meta property="og:url" content="http://youtube.github.io/vitess/documentation/concepts/">
	<meta property="og:site_name" content="Vitess">

    <link rel="canonical" href="http://youtube.github.io/vitess/documentation/concepts/">

    <link href="http://youtube.github.io/vitess/sitemap.xml" type="application/xml" rel="sitemap" title="Sitemap">

    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="cleartype" content="on">

    <style>
    .sliding-menu-content {
      top: 0;
      right: 0;
      text-align: center;
      visibility: hidden;
      height: 100%;
      width: 100%;
      -webkit-transform: translateX(100%);
      -moz-transform: translateX(100%);
      -ms-transform: translateX(100%);
      -o-transform: translateX(100%);
      transform: translateX(100%);
    }
    </style>

    <link rel="stylesheet" href="http://youtube.github.io/vitess/css/main.css">
    <!-- HTML5 Shiv and Media Query Support for IE -->
    <!--[if lt IE 9]>
      <script src="http://youtube.github.io/vitess/js/vendor/html5shiv.min.js"></script>
      <script src="http://youtube.github.io/vitess/js/vendor/respond.min.js"></script>
    <![endif]-->

  </head>

  <body>
    <header id="masthead">
  <div class="inner-wrap">
    <a href="http://youtube.github.io/vitess/" class="site-title">Vitess</a>
    <nav role="navigation" class="menu top-menu">
        <ul class="menu-item">
	<li class="home"><a href="/">Vitess</a></li>
	
    
        
    
    <li><a href="http://youtube.github.io/vitess/getting-started/" >Getting Started</a></li>
  
    
        
    
    <li><a href="http://youtube.github.io/vitess/documentation/" >Documentation</a></li>
  
    
        
    
    <li><a href="http://youtube.github.io/vitess/about/" >About</a></li>
  
    
        
    
    <li><a href="http://youtube.github.io/vitess/faq/" >FAQ</a></li>
  
</ul>
    </nav>
  </div>
</header>

    <nav role="navigation" class="js-menu sliding-menu-content">
	<ul class="menu-item">
		<li>
      
        
      
			<a href="http://youtube.github.io/vitess/getting-started/"><img src="http://youtube.github.io/vitess/images/400x250.gif" alt="teaser" class="teaser"></a>
			<a href="http://youtube.github.io/vitess/getting-started/" class="title">Getting Started</a>
			<p class="excerpt">Everything you need to know to get started with Vitess.</p>
		</li><li>
      
        
      
			<a href="http://youtube.github.io/vitess/documentation/"><img src="http://youtube.github.io/vitess/images/400x250.gif" alt="teaser" class="teaser"></a>
			<a href="http://youtube.github.io/vitess/documentation/" class="title">Documentation</a>
			<p class="excerpt">Vitess Docs.</p>
		</li><li>
      
        
      
			<a href="http://youtube.github.io/vitess/about/"><img src="http://youtube.github.io/vitess/images/400x250.gif" alt="teaser" class="teaser"></a>
			<a href="http://youtube.github.io/vitess/about/" class="title">About</a>
			<p class="excerpt">All about Vitess.</p>
		</li><li>
      
        
      
			<a href="http://youtube.github.io/vitess/faq/"><img src="http://youtube.github.io/vitess/images/400x250.gif" alt="teaser" class="teaser"></a>
			<a href="http://youtube.github.io/vitess/faq/" class="title">FAQ</a>
			<p class="excerpt">Vitess Faq.</p>
		</li>
	</ul>
</nav>
<button type="button" class="js-menu-trigger sliding-menu-button menulines-button x2" role="button" aria-label="Toggle Navigation">
	<span class="menulines"></span>
</button>

<div class="js-menu-screen menu-screen"></div>

    <div id="page-wrapper">
      <!--[if lt IE 9]><div class="upgrade notice-danger"><strong>Your browser is quite old!</strong> Why not <a href="http://whatbrowser.org/">upgrade to a newer one</a> to better enjoy this site?</div><![endif]-->

      <div id="main" role="main">
  <article class="wrap" itemscope itemtype="http://schema.org/Article">
    
    
  <nav class="breadcrumbs">
    <span itemscope itemtype="http://data-vocabulary.org/Breadcrumb">
      <a href="http://youtube.github.io/vitess" itemprop="url">
        <span itemprop="title">Home</span>
      </a> › 
    <span itemscope itemtype="http://data-vocabulary.org/Breadcrumb">
      <a href="http://youtube.github.io/vitess/documentation/" itemprop="url">
        <span itemprop="title">Documentation</span>
      </a>
    </span>
  </nav>


    <div class="inner-wrap">
      <nav class="toc"></nav>
      <div id="content" class="page-content" itemprop="articleBody">
	<h1 id="concepts">Concepts</h1>

<p>We need to introduce some common terminologies that are used in Vitess:</p>

<h3 id="keyspace">Keyspace</h3>

<p>A keyspace is a logical database.
In its simplest form, it directly maps to a MySQL database name.
When you read data from a keyspace, it is as if you read from a MySQL database.
Vitess could fetch that data from a master or a replica depending
on the consistency requirements of the read.</p>

<p>When a database gets <a href="http://en.wikipedia.org/wiki/Shard_(database_architecture)">sharded</a>,
a keyspace maps to multiple MySQL databases,
and the necessary data is fetched from one of the shards.
Reading from a keyspace gives you the impression that the data is read from
a single MySQL database.</p>

<h3 id="shard">Shard</h3>

<p>A division within a Keyspace. All the instances inside a Shard have the same data (or should have the same data,
modulo some replication lag).</p>

<p>A Keyspace usually has one shard when not using any sharding (we name it &#39;0&#39; by convention). When sharded, a Keyspace will have N shards (usually, N is a power of 2) with non-overlapping data.</p>

<p>We support <a href="Resharding.md">dynamic resharding</a>, when one shard is split into 2 shards for instance. In this case, the data in the
source shard is duplicated into the 2 destination shards, but only during the transition. Afterwards, the source shard is
deleted.</p>

<p>A shard usually contains one MySQL master, and many MySQL slaves. The slaves are used to serve read-only traffic (with
eventual consistency guarantees), run data analysis tools that take a long time to run, or perform administrative tasks (backups, restore, diffs, ...)</p>

<h3 id="tablet">Tablet</h3>

<p>A tablet is a single server that runs:
- a MySQL instance
- a vttablet instance
- a local row cache instance
- an other per-db process that is necessary for operational purposes</p>

<p>It can be idle (not assigned to any keyspace), or assigned to a keyspace/shard. If it becomes unhealthy, it is usually changed to scrap.</p>

<p>It has a type. The commonly used types are:
- master: for the mysql master, RW database.
- replica: for a mysql slave that serves read-only traffic, with guaranteed low replication latency.
- rdonly: for a mysql slave that serves read-only traffic for backend processing jobs (like map-reduce type jobs). It has no real guaranteed replication latency.
- spare: for a mysql slave not use at the moment (hot spare).
- experimental, schema, lag, backup, restore, checker, ... : various types for specific purposes.</p>

<p>Only master, replica and rdonly are advertised in the Serving Graph.</p>

<h3 id="keyspace-id">Keyspace id</h3>

<p>A keyspace id (keyspace_id) is a column that is used to identify a primary entity
of a keyspace, like user, video, order, etc.
In order to shard a database, all tables in a keyspace need to
contain a keyspace id column.
Vitess sharding ensures that all rows that have a common keyspace id are
always together.</p>

<p>It&#39;s recommended, but not necessary, that the keyspace id be the leading primary
key column of all tables in a keyspace.</p>

<p>If you do not intend to shard a database, you do not have to
designate a keyspace_id.
However, you&#39;ll be required to designate a keyspace_id
if you decide to shard a currently unsharded database.</p>

<p>A keyspace_id can be an unsigned number or a binary character column (unsigned bigint
or varbinary in mysql tables). Other data types are not allowed because of ambiguous
equality or inequality rules.</p>

<p>TODO: The keyspace id rules need to be solidified once VTGate features are finalized.</p>

<h3 id="shard-graph">Shard graph</h3>

<p>The shard graph defines how a keyspace has been sharded. It&#39;s basically a per-keyspace
list of non-intersecting ranges that cover all possible values a keyspace id can cover.
In other words, any given keypsace id is guaranteed to map to one and only one
shard of the shard graph.</p>

<p>We are going with range based sharding.
The main advantage of this scheme is that the shard map is a simple in-memory lookup.
The downside of this scheme is that it creates hot-spots for sequentially increasing keys.
In such cases, we recommend that the application hash the keys so they
distribute more randomly.</p>

<p>For instance, an application may use an incrementing UserId as a primary key for user records,
and a hashed version of that UserId as a keyspace_id. All data related to one user will be on
the same shard, as all rows will share that keyspace_id.</p>

<h3 id="replication-graph">Replication graph</h3>

<p>The <a href="ReplicationGraph.md">Replication Graph</a> represents the relationships between the master
databases and their respective replicas.
This data is particularly useful during a master failover.
Once a new master has been designated, all existing replicas have to
repointed to the new master so that replication can resume.</p>

<h3 id="serving-graph">Serving graph</h3>

<p>The <a href="ServingGraph.md">Serving Graph</a> is derived from the shard and replication graph.
It represens the list of active servers that are available to serve
queries.
VTGate (or smart clients) query the serving graph to find out which servers
they are allowed to send queries to.</p>

<h3 id="topology-server">Topology Server</h3>

<p>The Topology Server is the backend service used to store the Topology data, and provide a locking service. The implementation we use in the tree is based on Zookeeper. Each Zookeeper process is run on a single server, but may share that server with other processes.</p>

<p>There is a global instance of that service. It contains data that doesn&#39;t change often, and references other local instances. It may be replicated locally in each Data Center as read-only copies. (a Zookeeper instance with two master instances per cell and one or two replicas per cell is a good configuration).</p>

<p>There is one local instance of that service per Cell (Data Center). The goal is to transparently support a Cell going down. When that happens, we assume the client traffic is drained out of that Cell, and the system can survive
using the remaining Cells. (a Zookeeper instance running on 3 or 5 hosts locally is a good configuration).</p>

<p>The data is partitioned as follows:
- Keyspaces: global instance
- Shards: global instance
- Tablets: local instances
- Serving Graph: local instances
- Replication Graph: the master alias is in the global instance, the master-slave map is in the local cells.</p>

<p>Clients usually just read the local Serving Graph, therefore they only need the local instance to be up. Also, we provide a caching layer for Zookeeper, to survive local Zookeeper failures and scale read-only access dramatically.</p>

<h3 id="cell-(data-center)">Cell (Data Center)</h3>

<p>A Cell is a group of servers and network infrastructure collocated in an area. It is usually a full Data Center, or a subset of a full Data Center.</p>

<p>A Cell has an associated Topology Server, hosted in that Cell. Most information about the tablets in a cell is hosted in that cell&#39;s Topology Server. That way a Cell can be taken down and rebuilt as a unit, for instance.</p>

<p>We try to limit cross-cell traffic (both for data and metadata), and gracefully handle cell-level failures (like a Cell being cut off the network). Having the ability to route client traffic to Cells individually is a great feature to have
(but not provided by the Vitess software).</p>

	<hr />
	<footer class="page-footer">
	  


<div class="author-image">
	<img src="http://youtube.github.io/vitess/images/" alt="Vitess Team">
</div>
<div class="author-content">
	<h3 class="author-name" >Written by <a href="https://github.com/youtube/vitess" itemprop="author">Vitess Team</a></h3>
	<p class="author-bio"></p>
</div>

	  <div class="inline-btn">
	<a class="btn-social twitter" href="https://twitter.com/intent/tweet?text=Concepts&amp;url=http://youtube.github.io/vitess/documentation/concepts/&amp;via=" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i> Share on Twitter</a>
	<a class="btn-social facebook" href="https://www.facebook.com/sharer/sharer.php?u=http://youtube.github.io/vitess/documentation/concepts/" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i> Share on Facebook</a>
	<a class="btn-social google-plus"  href="https://plus.google.com/share?url=http://youtube.github.io/vitess/documentation/concepts/" target="_blank"><i class="fa fa-google-plus" aria-hidden="true"></i> Share on Google+</a>
</div>

	  <div class="page-meta">
	<p>Updated <time datetime="2015-01-01T00:00:00Z" itemprop="datePublished">January 01, 2015</time></p>
</div>

	</footer>
	<aside>
	  
	</aside>
      </div>
    </div>
  </article>
</div>


      <footer role="contentinfo" id="site-footer">
  <nav role="navigation" class="menu bottom-menu">
    <ul class="menu-item">
      
      
        
      
      <li><a href="http://youtube.github.io/vitess/" >Home</a></li>
      
      
        
      
      <li><a href="http://youtube.github.io/vitess/getting-started/" >Getting Started</a></li>
      
      
        
      
      <li><a href="http://youtube.github.io/vitess/about/" >About</a></li>
      
      
        
      
      <li><a href="http://youtube.github.io/vitess/faq/" >FAQ</a></li>
      
      
        
      
      <li><a href="http://youtube.github.io/vitess/terms/" >Terms</a></li>
      
    </ul>
  </nav>
  <p class="copyright">&#169; 2015 <a href="http://youtube.github.io/vitess">Vitess</a> powered by <a href="http://www.google.com">Google Inc</a>.</p>
</footer>

    </div>

    <script src="http://youtube.github.io/vitess/js/vendor/jquery-1.9.1.min.js"></script>
    <script src="http://youtube.github.io/vitess/js/main.js"></script>
    
    
    <script type="text/javascript">
      $('.toc').toc({
        'selectors': 'h2', //elements to use as headings
        'container': '.page-content', //element to find all selectors in
        'smoothScrolling': true, //enable or disable smooth scrolling on click
        'prefix': 'toc', //prefix for anchor tags and class names
        'onHighlight': function(el) {}, //called when a new section is highlighted 
        'highlightOnScroll': true, //add class to heading that is currently in focus
        'highlightOffset': 100, //offset to trigger the next headline
        'anchorName': function(i, heading, prefix) { //custom function for anchor name
          return prefix+i;
        },
        'headerText': function(i, heading, $heading) { //custom function building the header-item text
          return $heading.text();
        },
        'itemClass': function(i, heading, $heading, prefix) { //custom function for item class
          return $heading[0].tagName.toLowerCase();
        }
      });
    </script>
    

  </body>

</html>
